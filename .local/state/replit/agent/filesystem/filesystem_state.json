{"file_contents":{"CONTRIBUTING.md":{"content":"# Contributing to Microsoft Purview Security Analyzer\n\nThank you for your interest in contributing to the Microsoft Purview Security Analyzer! This document provides guidelines for contributing to the project.\n\n## Getting Started\n\n### Prerequisites\n\n- Python 3.8 or higher\n- Git\n- Basic knowledge of Streamlit, Pandas, and Python\n\n### Development Setup\n\n1. **Fork the repository** on GitHub\n2. **Clone your fork locally**:\n```bash\ngit clone https://github.com/yourusername/microsoft-purview-security-analyzer.git\ncd microsoft-purview-security-analyzer\n```\n3. **Create a virtual environment**:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n4. **Install dependencies**:\n```bash\npip install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura\n```\n5. **Download the GeoLite2 database** (see INSTALLATION.md)\n\n## How to Contribute\n\n### Reporting Bugs\n\n1. **Check existing issues** to avoid duplicates\n2. **Create a detailed bug report** including:\n   - Description of the problem\n   - Steps to reproduce\n   - Expected vs actual behavior\n   - Environment details (OS, Python version, etc.)\n   - Sample data files (if applicable and safe to share)\n\n### Suggesting Features\n\n1. **Open an issue** with the \"enhancement\" label\n2. **Describe the feature** in detail:\n   - Use case and benefits\n   - Proposed implementation approach\n   - Any potential drawbacks or considerations\n\n### Code Contributions\n\n#### 1. Create a Feature Branch\n```bash\ngit checkout -b feature/your-feature-name\n```\n\n#### 2. Make Your Changes\n\n**Code Style Guidelines:**\n- Follow PEP 8 Python style guidelines\n- Use meaningful variable and function names\n- Add comments for complex logic\n- Keep functions focused and modular\n\n**File Organization:**\n- `app_new.py`: Main Streamlit application\n- `security_analyzer.py`: Core analysis functions\n- Add new modules for substantial new features\n\n#### 3. Test Your Changes\n- Test with various file formats (CSV, Excel)\n- Verify the interactive map functionality\n- Check error handling with invalid data\n- Ensure the dark theme works correctly\n\n#### 4. Commit Your Changes\n```bash\ngit add .\ngit commit -m \"Add feature: descriptive commit message\"\n```\n\n**Commit Message Guidelines:**\n- Use present tense (\"Add feature\" not \"Added feature\")\n- Keep the first line under 50 characters\n- Add detailed description if needed after a blank line\n\n#### 5. Push and Create Pull Request\n```bash\ngit push origin feature/your-feature-name\n```\n\nThen create a pull request on GitHub with:\n- Clear title and description\n- Reference any related issues\n- Describe what you've changed and why\n\n## Code Areas Needing Help\n\n### High Priority\n- **Performance optimization** for large datasets\n- **Additional file format support** (JSON, XML)\n- **Enhanced anomaly detection** algorithms\n- **Export options** (PDF reports, Excel)\n\n### Medium Priority\n- **User interface improvements**\n- **Additional visualizations**\n- **Configuration options**\n- **Logging and debugging tools**\n\n### Low Priority\n- **Internationalization** (multiple languages)\n- **Plugin system** for custom analyzers\n- **API integration** with security tools\n\n## Development Guidelines\n\n### Security Considerations\n- Never commit sensitive data or API keys\n- Validate all user inputs\n- Use secure file handling practices\n- Follow OWASP guidelines for web applications\n\n### Performance Best Practices\n- Cache expensive operations when possible\n- Use vectorized operations with Pandas\n- Optimize database queries\n- Handle large files efficiently\n\n### UI/UX Guidelines\n- Maintain the dark theme consistency\n- Provide clear error messages\n- Add progress indicators for long operations\n- Ensure responsive design\n\n## Testing\n\n### Manual Testing Checklist\n- [ ] File upload with various formats\n- [ ] Map interaction and zooming\n- [ ] Data table filtering and sorting\n- [ ] Export functionality\n- [ ] Error handling with invalid files\n- [ ] Performance with large datasets\n\n### Automated Testing (Future)\nWe're planning to add automated tests. Contributions in this area are welcome!\n\n## Documentation\n\nWhen contributing, please:\n- Update README.md if changing functionality\n- Add docstrings to new functions\n- Update INSTALLATION.md for new dependencies\n- Create examples for new features\n\n## Release Process\n\nMaintainers will:\n1. Review pull requests\n2. Test thoroughly\n3. Merge approved changes\n4. Create release tags\n5. Update documentation\n\n## Questions and Support\n\n- **GitHub Issues**: For bug reports and feature requests\n- **Discussions**: For questions and general discussion\n- **Email**: For security concerns or private matters\n\n## License\n\nBy contributing, you agree that your contributions will be licensed under the MIT License.\n\n## Recognition\n\nContributors will be acknowledged in:\n- README.md contributors section\n- Release notes\n- Project documentation\n\nThank you for helping make this tool better for the security community!","size_bytes":4952},"DEPLOYMENT.md":{"content":"# Deployment Guide - Microsoft Purview Security Analyzer\n\nThis guide covers various deployment options for the Microsoft Purview Security Analyzer.\n\n## Local Development\n\n### Quick Start\n```bash\ngit clone https://github.com/yourusername/microsoft-purview-security-analyzer.git\ncd microsoft-purview-security-analyzer\npip install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura\nstreamlit run app_new.py\n```\n\n## Production Deployment Options\n\n### 1. Streamlit Cloud (Recommended)\n\n**Pros**: Free, easy setup, automatic HTTPS, custom domains\n**Cons**: Resource limitations, public repositories only\n\n**Steps**:\n1. Push your code to GitHub\n2. Visit [share.streamlit.io](https://share.streamlit.io)\n3. Connect your GitHub repository\n4. Deploy with one click\n\n**Requirements**:\n- Public GitHub repository\n- Add GeoLite2-City.mmdb to .gitignore (users download separately)\n- Configure secrets for any API keys\n\n### 2. Heroku\n\n**Pros**: Reliable, scalable, add-ons available\n**Cons**: Paid service, requires some configuration\n\n**Setup**:\n```bash\n# Create Procfile\necho \"web: streamlit run app_new.py --server.port \\$PORT --server.address 0.0.0.0\" > Procfile\n\n# Create runtime.txt\necho \"python-3.11.0\" > runtime.txt\n\n# Deploy\nheroku create your-app-name\ngit push heroku main\n```\n\n### 3. Docker Container\n\n**Pros**: Consistent environment, works anywhere\n**Cons**: Requires Docker knowledge\n\n**Dockerfile**:\n```dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura\n\nEXPOSE 8501\n\nCMD [\"streamlit\", \"run\", \"app_new.py\", \"--server.address\", \"0.0.0.0\"]\n```\n\n**Build and run**:\n```bash\ndocker build -t purview-analyzer .\ndocker run -p 8501:8501 purview-analyzer\n```\n\n### 4. AWS EC2\n\n**Pros**: Full control, scalable, professional\n**Cons**: Requires AWS knowledge, costs money\n\n**Setup on Ubuntu EC2**:\n```bash\n# Update system\nsudo apt update && sudo apt upgrade -y\n\n# Install Python and pip\nsudo apt install python3 python3-pip -y\n\n# Clone and setup\ngit clone https://github.com/yourusername/microsoft-purview-security-analyzer.git\ncd microsoft-purview-security-analyzer\npip3 install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura\n\n# Run with nohup for background execution\nnohup streamlit run app_new.py --server.address 0.0.0.0 --server.port 8501 &\n```\n\n### 5. Google Cloud Platform\n\n**Pros**: Google infrastructure, auto-scaling\n**Cons**: Complex setup, costs money\n\nUse Google Cloud Run or App Engine for deployment.\n\n## Configuration for Production\n\n### Environment Variables\nCreate `.env` file for production settings:\n```bash\nSTREAMLIT_SERVER_PORT=8501\nSTREAMLIT_SERVER_ADDRESS=0.0.0.0\nSTREAMLIT_THEME_BASE=dark\n```\n\n### Performance Optimization\n\n1. **Memory Management**:\n   - Monitor memory usage with large files\n   - Implement file size limits if needed\n   - Use chunked processing for very large datasets\n\n2. **Caching**:\n   - Streamlit's @st.cache_data is already implemented\n   - Consider Redis for multi-user deployments\n\n3. **Security**:\n   - Use HTTPS in production\n   - Implement file upload limits\n   - Validate all user inputs\n   - Consider authentication for sensitive deployments\n\n### Load Balancing\n\nFor high-traffic deployments:\n\n**Nginx Configuration**:\n```nginx\nupstream streamlit {\n    server 127.0.0.1:8501;\n    server 127.0.0.1:8502;\n    server 127.0.0.1:8503;\n}\n\nserver {\n    listen 80;\n    server_name your-domain.com;\n    \n    location / {\n        proxy_pass http://streamlit;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n```\n\n## Monitoring and Maintenance\n\n### Health Checks\nAdd health check endpoint in Streamlit:\n```python\n# Add to app_new.py\nif st.query_params.get(\"health\") == \"check\":\n    st.write(\"OK\")\n    st.stop()\n```\n\n### Logging\n```python\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n```\n\n### Updates\n- Monitor dependencies for security updates\n- Test new Streamlit versions before upgrading\n- Backup user data and configurations\n\n## Troubleshooting Common Issues\n\n### Memory Errors\n- Increase server memory\n- Implement file size limits\n- Use streaming for large files\n\n### Port Issues\n- Check firewall settings\n- Ensure port is open in security groups (AWS)\n- Use environment variables for port configuration\n\n### SSL/HTTPS Issues\n- Use reverse proxy (Nginx/Apache)\n- Configure SSL certificates\n- Update Streamlit server settings\n\n## Cost Optimization\n\n### Free Options\n1. **Streamlit Cloud**: Best for open-source projects\n2. **Heroku Free Tier**: Limited hours but functional\n3. **GitHub Codespaces**: Development and testing\n\n### Paid Optimizations\n1. **Right-size instances**: Don't over-provision\n2. **Auto-scaling**: Scale down during low usage\n3. **CDN**: For static assets and global reach\n\n## Security Considerations\n\n### Data Protection\n- Never log sensitive information\n- Encrypt data in transit and at rest\n- Implement proper access controls\n\n### GDPR Compliance\n- Add privacy policy if handling EU data\n- Implement data deletion capabilities\n- Document data processing activities\n\n### Regular Security Updates\n- Monitor CVE databases\n- Update dependencies regularly\n- Perform security audits\n\n## Support and Maintenance\n\n### Backup Strategy\n- Code: GitHub repository\n- Data: Regular database backups if applicable\n- Configuration: Document all settings\n\n### Update Process\n1. Test updates in staging environment\n2. Schedule maintenance windows\n3. Have rollback plan ready\n4. Monitor post-deployment\n\nFor questions about deployment, please open an issue on the GitHub repository.","size_bytes":5649},"INSTALLATION.md":{"content":"# Installation Guide - Microsoft Purview Security Analyzer\n\n## Quick Start\n\n### Option 1: Using pip (Recommended)\n\n1. **Clone the repository**:\n```bash\ngit clone https://github.com/yourusername/microsoft-purview-security-analyzer.git\ncd microsoft-purview-security-analyzer\n```\n\n2. **Create a virtual environment** (optional but recommended):\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. **Install dependencies**:\n```bash\npip install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura\n```\n\n### Option 2: Using requirements file\n\nIf you create your own requirements.txt file with the following content:\n```\nstreamlit>=1.28.0\npandas>=2.0.0\nfolium>=0.14.0\nstreamlit-folium>=0.15.0\ngeoip2>=4.7.0\nopenpyxl>=3.1.0\ntrafilatura>=1.6.0\n```\n\nThen install with:\n```bash\npip install -r requirements.txt\n```\n\n## Data Files\n\n### GeoLite2 Database\n\nThe GeoLite2-City.mmdb database file is **included** in the repository for your convenience. No additional download is required!\n\n**File structure**:\n```\nmicrosoft-purview-security-analyzer/\nâ”œâ”€â”€ attached_assets/\nâ”‚   â””â”€â”€ GeoLite2-City.mmdb  # Included in repository\nâ”œâ”€â”€ app_new.py\nâ”œâ”€â”€ security_analyzer.py\nâ””â”€â”€ ...\n```\n\n## Running the Application\n\n1. **Start the application**:\n```bash\nstreamlit run app_new.py\n```\n\n2. **Access the web interface**:\n   - Open your browser and go to `http://localhost:8501`\n   - The application will automatically open in your default browser\n\n## Troubleshooting\n\n### Common Issues\n\n**1. \"ModuleNotFoundError\" when running**\n- Make sure all dependencies are installed: `pip install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura`\n\n**2. \"FileNotFoundError: GeoLite2-City.mmdb\"**\n- The database file should be included in the repository\n- Verify the file exists at: `attached_assets/GeoLite2-City.mmdb`\n- If missing, you can download it from [MaxMind GeoLite2](https://dev.maxmind.com/geoip/geoip2/geolite2/)\n\n**3. \"Port already in use\"**\n- If port 8501 is busy, Streamlit will automatically use the next available port\n- Check the terminal output for the correct URL\n\n**4. Application won't start**\n- Ensure you're in the correct directory\n- Check that Python 3.8+ is installed: `python --version`\n- Try running with: `python -m streamlit run app_new.py`\n\n### System Requirements\n\n- **Python**: 3.8 or higher\n- **Operating System**: Windows, macOS, or Linux\n- **Memory**: At least 2GB RAM recommended\n- **Storage**: 500MB for dependencies and database files\n\n### Performance Tips\n\n- For large files (>100MB), consider increasing system memory\n- Close other browser tabs when processing large datasets\n- Use SSD storage for better file processing performance\n\n## Development Setup\n\nIf you plan to modify or contribute to the project:\n\n1. **Fork the repository** on GitHub\n2. **Clone your fork**:\n```bash\ngit clone https://github.com/yourusername/microsoft-purview-security-analyzer.git\n```\n3. **Create a development branch**:\n```bash\ngit checkout -b feature/your-feature-name\n```\n4. **Make your changes and test**\n5. **Submit a pull request**\n\n## Deployment Options\n\n### Local Network Access\nTo allow access from other devices on your network:\n```bash\nstreamlit run app_new.py --server.address 0.0.0.0\n```\n\n### Production Deployment\nFor production deployment, consider:\n- [Streamlit Cloud](https://streamlit.io/cloud)\n- [Heroku](https://heroku.com)\n- [AWS](https://aws.amazon.com)\n- [Docker containers](https://docker.com)\n\n## Support\n\nIf you encounter issues:\n1. Check this installation guide\n2. Review the main [README.md](README.md)\n3. Open an issue on the GitHub repository","size_bytes":3663},"README.md":{"content":"# Microsoft Purview Security Analyzer\n\nA web-based security analysis tool that leverages Streamlit to process CSV/Excel files and visualize anomalous IP activities with interactive mapping capabilities.\n\n## Features\n\n- **Interactive Geospatial Mapping**: Visualize suspicious IP activities on an interactive world map\n- **File Processing**: Support for CSV and Excel files containing audit logs\n- **Anomaly Detection**: Identify potentially compromised events and anomalous IP addresses\n- **Data Visualization**: Interactive tables and charts for detailed analysis\n- **Export Functionality**: Download analysis results in JSON format\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8 or higher\n- pip package manager\n\n### Setup\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/yourusername/microsoft-purview-security-analyzer.git\ncd microsoft-purview-security-analyzer\n```\n\n2. Install required packages:\n```bash\npip install streamlit pandas folium streamlit-folium geoip2 openpyxl trafilatura\n```\n\n3. The GeoLite2 database is included in the repository for convenience\n\n## Usage\n\n1. Start the application:\n```bash\nstreamlit run app_new.py\n```\n\n2. Open your web browser and navigate to `http://localhost:8501`\n\n3. Upload your CSV or Excel file containing audit logs\n\n4. Analyze the results in the interactive dashboard\n\n## Expected Data Format\n\nThe uploaded file should contain audit log data with at least the following columns:\n- **CreationDate**: Timestamp of the event\n- **Operation**: Type of operation performed\n- **UserId**: User identifier\n- **AuditData**: JSON data containing details like ClientIP\n\nAdditional columns will be utilized if available.\n\n## File Structure\n\n```\nmicrosoft-purview-security-analyzer/\nâ”œâ”€â”€ app_new.py                 # Main Streamlit application\nâ”œâ”€â”€ security_analyzer.py      # Core analysis functions\nâ”œâ”€â”€ attached_assets/           # Data files and assets\nâ”‚   â””â”€â”€ GeoLite2-City.mmdb   # GeoIP database (included)\nâ”œâ”€â”€ .streamlit/\nâ”‚   â””â”€â”€ config.toml           # Streamlit configuration\nâ”œâ”€â”€ pyproject.toml            # Python dependencies\nâ””â”€â”€ README.md                 # This file\n```\n\n## Analysis Capabilities\n\nThis tool provides:\n\n1. **Anomalous IP Detection**: Identify IP addresses with unusual geographic patterns\n2. **Compromised Event Detection**: Flag potentially suspicious activities\n3. **File Access Tracking**: Monitor file operations and access patterns\n4. **Interactive Mapping**: Visualize geographic distribution of security events\n5. **Detailed Reporting**: Generate comprehensive analysis reports\n\n## Technologies Used\n\n- **Streamlit**: Web application framework\n- **Folium**: Interactive mapping\n- **Pandas**: Data manipulation and analysis\n- **GeoIP2**: IP geolocation services\n- **Trafilatura**: Web content extraction\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n### Third-Party Data\nThis product includes GeoLite2 data created by MaxMind, available from [https://www.maxmind.com](https://www.maxmind.com). The GeoLite2 database is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## Disclaimer\n\nThis tool is for educational and legitimate security analysis purposes only. Users are responsible for ensuring compliance with all applicable laws and regulations when analyzing data.\n\n## Support\n\nFor questions or issues, please open an issue on the GitHub repository.","size_bytes":3801},"app.py":{"content":"import streamlit as st\nimport pandas as pd\nimport folium\nfrom streamlit_folium import folium_static\nimport tempfile\nimport os\nimport json\nfrom utils import parse_audit_logs, detect_compromised_events, filter_files_accessed, filter_anomalous_ips\n\n# Set page configuration\nst.set_page_config(\n    page_title=\"Security Audit Log Analyzer\",\n    page_icon=\"ðŸ”’\",\n    layout=\"wide\"\n)\n\n# Title and description\nst.title(\"ðŸ”’ Security Audit Log Analyzer\")\nst.markdown(\"\"\"\nThis tool analyzes audit logs to identify anomalous IP activities and potential security breaches.\nUpload your CSV or Excel file containing the audit logs to get started.\n\"\"\")\n\n# Sidebar for file upload and options\nwith st.sidebar:\n    st.header(\"Upload Data\")\n    uploaded_file = st.file_uploader(\"Choose a CSV or Excel file\", type=[\"csv\", \"xlsx\", \"xls\"])\n    \n    st.header(\"GeoIP Database\")\n    geoip_file = st.file_uploader(\"Upload GeoLite2-City.mmdb (optional)\", type=[\"mmdb\"])\n    \n    if uploaded_file is not None:\n        st.success(\"File uploaded successfully!\")\n        \n        # Save the GeoIP database to a temporary file if uploaded\n        geoip_db_path = None\n        if geoip_file is not None:\n            temp_dir = tempfile.mkdtemp()\n            geoip_db_path = os.path.join(temp_dir, \"GeoLite2-City.mmdb\")\n            with open(geoip_db_path, \"wb\") as f:\n                f.write(geoip_file.getbuffer())\n            st.success(\"GeoIP database loaded!\")\n\n# Main content area\nif uploaded_file is not None:\n    # Process the uploaded file\n    try:\n        file_extension = uploaded_file.name.split(\".\")[-1].lower()\n        \n        # Save the uploaded file to a temporary location\n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_file:\n            tmp_file.write(uploaded_file.getbuffer())\n            temp_filepath = tmp_file.name\n        \n        with st.spinner(\"Analyzing audit logs...\"):\n            # Process the file based on its extension\n            timeline = parse_audit_logs(temp_filepath, geoip_db_path)\n            compromised_events = detect_compromised_events(timeline)\n            files_accessed = filter_files_accessed(timeline)\n            anomalous_ips = filter_anomalous_ips(timeline)\n        \n        # Clean up temporary file\n        os.unlink(temp_filepath)\n        \n        # Display summary metrics\n        col1, col2, col3, col4 = st.columns(4)\n        with col1:\n            st.metric(\"Total Events\", len(timeline))\n        with col2:\n            st.metric(\"Suspicious Events\", len(compromised_events))\n        with col3:\n            st.metric(\"Files Accessed\", len(files_accessed))\n        with col4:\n            st.metric(\"Anomalous IPs\", len({event['ClientIP'] for event in anomalous_ips}))\n        \n        # Create tabs for different views\n        tab1, tab2, tab3, tab4 = st.tabs([\"Map View\", \"Suspicious Events\", \"Files Accessed\", \"Timeline\"])\n        \n        with tab1:\n            st.header(\"Anomalous IP Locations\")\n            \n            # Create a map centered at a default location\n            m = folium.Map(location=[40, 0], zoom_start=2)\n            \n            # Add markers for anomalous IPs\n            unique_ips = {}\n            for event in anomalous_ips:\n                ip = event['ClientIP']\n                country = event['Country']\n                region = event['Region']\n                \n                # Skip if IP is Unknown or missing geo data\n                if ip == 'N/A' or country == \"Unknown\":\n                    continue\n                \n                # Create a key for each unique location to avoid duplicate markers\n                key = f\"{ip}_{country}_{region}\"\n                if key not in unique_ips:\n                    unique_ips[key] = {\n                        'ip': ip,\n                        'country': country,\n                        'region': region,\n                        'events': []\n                    }\n                unique_ips[key]['events'].append(event)\n            \n            # Add markers to the map\n            for location_key, location_data in unique_ips.items():\n                # Get first event to extract geo coordinates (assuming all events for this IP have same coords)\n                first_event = location_data['events'][0]\n                \n                # Prepare popup content\n                popup_content = f\"\"\"\n                <b>IP:</b> {location_data['ip']}<br>\n                <b>Country:</b> {location_data['country']}<br>\n                <b>Region:</b> {location_data['region']}<br>\n                <b>Events:</b> {len(location_data['events'])}<br>\n                <hr>\n                <b>Events Details:</b><br>\n                \"\"\"\n                \n                for i, event in enumerate(location_data['events'][:5]):  # Limit to first 5 events to avoid huge popups\n                    popup_content += f\"\"\"\n                    <b>Event {i+1}:</b><br>\n                    - Time: {event['Timestamp']}<br>\n                    - Operation: {event['Operation']}<br>\n                    - User: {event['UserId']}<br>\n                    \"\"\"\n                    if event.get('FileName'):\n                        popup_content += f\"- File: {event['FileName']}<br>\"\n                        \n                if len(location_data['events']) > 5:\n                    popup_content += f\"<i>...and {len(location_data['events']) - 5} more events</i>\"\n                \n                # Add marker to map\n                folium.Marker(\n                    # For demo purposes, we'll generate coordinates based on the country code\n                    # In a real implementation, you would use actual lat/long from the GeoIP lookup\n                    location=[\n                        40 + hash(location_data['country']) % 30 - 15,  # Pseudo-random latitude\n                        hash(location_data['region']) % 60 - 30         # Pseudo-random longitude\n                    ],\n                    popup=folium.Popup(popup_content, max_width=300),\n                    tooltip=f\"{location_data['ip']} ({location_data['country']})\",\n                    icon=folium.Icon(color='red', icon='info-sign')\n                ).add_to(m)\n            \n            # Display the map\n            folium_static(m)\n            \n            # Display table of anomalous IPs\n            st.subheader(\"Anomalous IP Details\")\n            if anomalous_ips:\n                df_anomalous = pd.DataFrame(anomalous_ips)\n                st.dataframe(df_anomalous, use_container_width=True)\n            else:\n                st.info(\"No anomalous IPs detected in the dataset.\")\n        \n        with tab2:\n            st.header(\"Suspicious Events\")\n            if compromised_events:\n                df_compromised = pd.DataFrame(compromised_events)\n                st.dataframe(df_compromised, use_container_width=True)\n            else:\n                st.info(\"No suspicious events detected in the dataset.\")\n        \n        with tab3:\n            st.header(\"Files Accessed Events\")\n            if files_accessed:\n                df_files = pd.DataFrame(files_accessed)\n                st.dataframe(df_files, use_container_width=True)\n            else:\n                st.info(\"No file access events found in the dataset.\")\n        \n        with tab4:\n            st.header(\"Complete Timeline\")\n            if timeline:\n                df_timeline = pd.DataFrame(timeline)\n                st.dataframe(df_timeline, use_container_width=True)\n            else:\n                st.info(\"No timeline events found in the dataset.\")\n        \n        # Add download button for the results\n        st.subheader(\"Download Results\")\n        \n        # Create a temporary Excel file with the results\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp_xlsx:\n            with pd.ExcelWriter(tmp_xlsx.name, engine='openpyxl') as writer:\n                pd.DataFrame(timeline).to_excel(writer, sheet_name='Timeline', index=False)\n                pd.DataFrame(compromised_events).to_excel(writer, sheet_name='Suspicious Events', index=False)\n                pd.DataFrame(files_accessed).to_excel(writer, sheet_name='FilesAccessed', index=False)\n                pd.DataFrame(anomalous_ips).to_excel(writer, sheet_name='AnomalousIPs', index=False)\n            \n            # Provide download button\n            with open(tmp_xlsx.name, \"rb\") as f:\n                excel_data = f.read()\n                st.download_button(\n                    label=\"Download Excel Report\",\n                    data=excel_data,\n                    file_name=\"security_analysis_report.xlsx\",\n                    mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n                )\n            \n            # Clean up the temporary file\n            os.unlink(tmp_xlsx.name)\n        \n    except Exception as e:\n        st.error(f\"Error processing file: {str(e)}\")\n        st.exception(e)\nelse:\n    # Display instructions when no file is uploaded\n    st.info(\"ðŸ“¤ Please upload a CSV or Excel file containing audit logs to begin analysis.\")\n    \n    # Display example data structure\n    st.subheader(\"Expected Data Structure\")\n    st.markdown(\"\"\"\n    The uploaded file should contain audit logs with the following columns:\n    \n    - **CreationDate**: The timestamp of the event\n    - **Operation**: The operation performed (e.g., FileAccessed, SoftDelete)\n    - **UserId**: The user ID associated with the event\n    - **AuditData**: JSON data containing details like ClientIP, ResultStatus, etc.\n    \n    For optimal results, also upload the GeoLite2-City.mmdb database for IP geolocation.\n    \"\"\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"ðŸ”’ Security Audit Log Analyzer - Helping IT teams identify potential security breaches\")\n","size_bytes":9769},"app_new.py":{"content":"import streamlit as st\nimport pandas as pd\nimport folium\nfrom streamlit_folium import st_folium\nimport tempfile\nimport os\nimport json\nfrom security_analyzer import parse_audit_logs, detect_compromised_events, filter_files_accessed, filter_anomalous_ips\n\n# Initialize session state variables to store processed data between re-renders\nif 'processed' not in st.session_state:\n    st.session_state.processed = False\nif 'timeline' not in st.session_state:\n    st.session_state.timeline = None\nif 'compromised_events' not in st.session_state:\n    st.session_state.compromised_events = None\nif 'files_accessed' not in st.session_state:\n    st.session_state.files_accessed = None\nif 'anomalous_ips' not in st.session_state:\n    st.session_state.anomalous_ips = None\n\n# Set page configuration\nst.set_page_config(\n    page_title=\"Microsoft Purview Security Analyzer\",\n    page_icon=\"ðŸ”’\",\n    layout=\"wide\"\n)\n\n# Title and description\nst.title(\"Microsoft Purview Security Analyzer\")\nst.markdown(\"\"\"\nThis tool helps identify anomalous IP activities and potentially compromised events in audit logs.\nUpload your CSV or Excel file to analyze the data and visualize suspicious activities on the map.\n\"\"\")\n\n# Add file upload widget\nuploaded_file = st.file_uploader(\"Upload CSV or Excel file\", type=[\"csv\", \"xlsx\", \"xls\"])\n\n# Function to save uploaded file temporarily\ndef save_uploaded_file(uploaded_file):\n    try:\n        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:\n            tmp.write(uploaded_file.getvalue())\n            return tmp.name\n    except Exception as e:\n        st.error(f\"Error saving file: {e}\")\n        return None\n\n# Function to create map with anomalous IPs\ndef create_map(anomalous_ips):\n    # Create a map centered at a default location\n    m = folium.Map(location=[20, 0], zoom_start=2)\n    \n    # Add markers for each anomalous IP\n    ip_locations = {}\n    \n    for idx, event in enumerate(anomalous_ips):\n        # Skip if Country or Region is unknown\n        if event.get('Country') == 'Unknown' or event.get('Region') == 'Unknown':\n            continue\n        \n        # Get location and create a unique key\n        lat = event.get('Latitude', 0)\n        lon = event.get('Longitude', 0)\n        \n        if lat == 0 and lon == 0:\n            continue\n            \n        location_key = f\"{lat}_{lon}\"\n        \n        # Check if we already have a marker at this location\n        if location_key in ip_locations:\n            ip_locations[location_key]['count'] += 1\n            ip_locations[location_key]['events'].append(event)\n        else:\n            ip_locations[location_key] = {\n                'lat': lat,\n                'lon': lon,\n                'count': 1,\n                'events': [event]\n            }\n    \n    # Add markers for each unique location\n    for location_key, location_data in ip_locations.items():\n        # Create popup with all events at this location\n        popup_content = \"<div style='width: 300px'><h4>Location Events</h4>\"\n        for idx, event in enumerate(location_data['events']):\n            popup_content += f\"\"\"\n            <div style=\"margin-bottom: 10px; border-bottom: 1px solid #ccc; padding-bottom: 5px;\">\n                <p><b>Event {idx+1}</b></p>\n                <p><b>IP Address:</b> {event.get('ClientIP', 'N/A')}</p>\n                <p><b>Timestamp:</b> {event.get('Timestamp', 'N/A')}</p>\n                <p><b>Operation:</b> {event.get('Operation', 'N/A')}</p>\n                <p><b>User ID:</b> {event.get('UserId', 'N/A')}</p>\n                <p><b>Location:</b> {event.get('Region', 'Unknown')}, {event.get('Country', 'Unknown')}</p>\n            </div>\n            \"\"\"\n        popup_content += \"</div>\"\n        \n        # Add marker with the count of events\n        folium.Marker(\n            location=[location_data['lat'], location_data['lon']],\n            popup=folium.Popup(popup_content, max_width=350),\n            tooltip=f\"{location_data['count']} event(s) at this location\",\n            icon=folium.Icon(color='red', icon='info-sign')\n        ).add_to(m)\n    \n    return m\n\n# Function to process the data (only runs when we need to)\ndef process_data(file_path):\n    # Set up a progress bar for file processing\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    # Process the file with progress updates\n    status_text.text(\"Reading and parsing the file...\")\n    progress_bar.progress(10)\n    \n    # Parse the audit logs with our optimized geolocation service\n    timeline = parse_audit_logs(file_path)\n    \n    if not timeline:\n        progress_bar.progress(100)\n        status_text.text(\"\")\n        st.error(\"No data could be processed from the file. Please check the file format.\")\n        return False\n    \n    status_text.text(\"Processing IP geolocation data...\")\n    progress_bar.progress(50)\n    \n    # Continue with analysis\n    status_text.text(\"Analyzing anomalies and generating insights...\")\n    progress_bar.progress(80)\n    \n    # Extract insights\n    compromised_events = detect_compromised_events(timeline)\n    files_accessed = filter_files_accessed(timeline)\n    anomalous_ips = filter_anomalous_ips(timeline)\n    \n    # Store results in session state\n    st.session_state.timeline = timeline\n    st.session_state.compromised_events = compromised_events\n    st.session_state.files_accessed = files_accessed\n    st.session_state.anomalous_ips = anomalous_ips\n    st.session_state.processed = True\n    \n    # Complete the progress bar\n    progress_bar.progress(100)\n    status_text.text(\"\")\n    st.success(f\"Successfully processed {len(timeline)} events.\")\n    return True\n\n# Main application logic\nif uploaded_file is not None:\n    try:\n        # Reset session state if a new file is uploaded\n        current_file = getattr(st.session_state, 'current_file', None) \n        if current_file != uploaded_file.name:\n            st.session_state.processed = False\n            st.session_state.current_file = uploaded_file.name\n        \n        # Only process the file if we haven't already done so for this file\n        if not st.session_state.processed:\n            # Save uploaded file temporarily\n            temp_file_path = save_uploaded_file(uploaded_file)\n            # Process the file only if we have a valid file path\n            if temp_file_path:\n                process_data(temp_file_path)\n                # Clean up temporary file when done\n                if os.path.exists(temp_file_path):\n                    os.unlink(temp_file_path)\n            else:\n                st.error(\"Failed to save uploaded file. Please try again.\")\n        \n        # If processing was successful, display the results\n        if st.session_state.processed and st.session_state.timeline is not None:\n            # Get data from session state\n            timeline = st.session_state.timeline\n            compromised_events = st.session_state.compromised_events or []\n            files_accessed = st.session_state.files_accessed or []\n            anomalous_ips = st.session_state.anomalous_ips or []\n            \n            # Display summary statistics\n            col1, col2, col3, col4 = st.columns(4)\n            col1.metric(\"Total Events\", len(timeline))\n            col2.metric(\"Suspicious Events\", len(compromised_events))\n            col3.metric(\"Files Accessed\", len(files_accessed))\n            col4.metric(\"Anomalous IPs\", len(anomalous_ips))\n            \n            # Create tabs for different views\n            tab1, tab2, tab3, tab4 = st.tabs([\"Map View\", \"Suspicious Events\", \"Files Accessed\", \"All Events\"])\n            \n            with tab1:\n                st.subheader(\"Anomalous IP Activity Map\")\n                st.markdown(\"Red markers indicate locations with anomalous IP activity. Click on a marker to see details.\")\n                \n                # Create and display the map\n                if anomalous_ips:\n                    map_data = create_map(anomalous_ips)\n                    st_folium(map_data, width=1200, height=600)\n                else:\n                    st.info(\"No anomalous IP activity detected in the data.\")\n                \n                # Display anomalous IP events in a table\n                if anomalous_ips:\n                    st.subheader(\"Anomalous IP Events\")\n                    # Convert to DataFrame for better display\n                    anomalous_df = pd.DataFrame(anomalous_ips)\n                    # Format timestamp column\n                    if 'Timestamp' in anomalous_df.columns:\n                        anomalous_df['Timestamp'] = pd.to_datetime(anomalous_df['Timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n                    st.dataframe(anomalous_df)\n            \n            with tab2:\n                st.subheader(\"Suspicious Events\")\n                if compromised_events:\n                    compromised_df = pd.DataFrame(compromised_events)\n                    # Format timestamp column\n                    if 'Timestamp' in compromised_df.columns:\n                        compromised_df['Timestamp'] = pd.to_datetime(compromised_df['Timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n                    st.dataframe(compromised_df)\n                else:\n                    st.info(\"No suspicious events detected in the data.\")\n            \n            with tab3:\n                st.subheader(\"Files Accessed\")\n                if files_accessed:\n                    files_df = pd.DataFrame(files_accessed)\n                    # Format timestamp column\n                    if 'Timestamp' in files_df.columns:\n                        files_df['Timestamp'] = pd.to_datetime(files_df['Timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n                    st.dataframe(files_df)\n                else:\n                    st.info(\"No file access events found in the data.\")\n            \n            with tab4:\n                st.subheader(\"All Events\")\n                timeline_df = pd.DataFrame(timeline)\n                # Format timestamp column\n                if 'Timestamp' in timeline_df.columns:\n                    timeline_df['Timestamp'] = pd.to_datetime(timeline_df['Timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n                st.dataframe(timeline_df)\n            \n            # Option to download results\n            st.subheader(\"Download Results\")\n            \n            # Prepare data for download\n            output = {\n                \"timeline\": timeline,\n                \"compromised_events\": compromised_events,\n                \"files_accessed\": files_accessed,\n                \"anomalous_ips\": anomalous_ips\n            }\n            \n            # Convert to JSON for download\n            json_data = json.dumps(output, default=str)\n            st.download_button(\n                label=\"Download Results as JSON\",\n                data=json_data,\n                file_name=\"security_analysis_results.json\",\n                mime=\"application/json\"\n            )\n    \n    except Exception as e:\n        st.error(f\"An error occurred during processing: {str(e)}\")\n        import traceback\n        st.error(traceback.format_exc())\nelse:\n    # Display instructions when no file is uploaded\n    st.info(\"Please upload a CSV or Excel file containing audit logs to begin analysis.\")\n    \n    # Show example of expected data format\n    st.subheader(\"Expected Data Format\")\n    st.markdown(\"\"\"\n    The uploaded file should contain audit log data with at least the following columns:\n    - CreationDate: Timestamp of the event\n    - Operation: Type of operation performed\n    - UserId: User identifier\n    - AuditData: JSON data containing details like ClientIP\n    \n    Other columns will be used if available but are not required.\n    \"\"\")\n    \n    # Show sample of the analysis capabilities\n    st.subheader(\"Analysis Capabilities\")\n    st.markdown(\"\"\"\n    This tool will:\n    1. Identify anomalous IP addresses based on geolocation\n    2. Detect potentially compromised events\n    3. Track file access operations\n    4. Visualize suspicious activities on an interactive map\n    5. Provide detailed tables of all events for further investigation\n    \"\"\")","size_bytes":12113},"processor.py":{"content":"import pandas as pd\nimport json\nimport geoip2.database\nimport os\nimport tempfile\nfrom datetime import datetime\nimport io\n\ndef parse_audit_logs(file_content, file_type, geoip_db_path=None):\n    \"\"\"\n    Parse audit logs from an uploaded file (CSV or Excel).\n    \"\"\"\n    try:\n        # Read data based on file type\n        if file_type == 'csv':\n            df = pd.read_csv(io.BytesIO(file_content))\n        else:  # Excel\n            df = pd.read_excel(io.BytesIO(file_content))\n\n        # Select relevant columns\n        columns_of_interest = ['CreationDate', 'Operation', 'UserId', 'AuditData']\n        \n        # Check if all required columns exist\n        missing_columns = [col for col in columns_of_interest if col not in df.columns]\n        if missing_columns:\n            return None, f\"Missing required columns: {', '.join(missing_columns)}\"\n        \n        df = df[columns_of_interest]\n\n        # Convert CreationDate to datetime for sorting\n        df['CreationDate'] = pd.to_datetime(df['CreationDate'])\n        df = df.sort_values(by='CreationDate')\n\n        # Open the GeoLite2 database if available\n        geo_reader = None\n        if geoip_db_path and os.path.exists(geoip_db_path):\n            try:\n                geo_reader = geoip2.database.Reader(geoip_db_path)\n            except Exception as ge:\n                return None, f\"Error loading GeoIP database: {str(ge)}\"\n        else:\n            return None, \"GeoIP database not found\"\n\n        # Parse AuditData and create a timeline with geolocation data\n        timeline = []\n        error_count = 0\n        \n        for _, row in df.iterrows():\n            try:\n                audit_data = json.loads(row['AuditData'])\n                event = {\n                    'Timestamp': row['CreationDate'],\n                    'Operation': row['Operation'],\n                    'UserId': row['UserId'],\n                    'ClientIP': audit_data.get('ClientIP', 'N/A'),\n                    'ResultStatus': audit_data.get('ResultStatus', 'N/A')\n                }\n                \n                # For FileAccessed events, extract the file name if available\n                if row['Operation'] == 'FileAccessed':\n                    event['FileName'] = audit_data.get('SourceFileName', 'N/A')\n                else:\n                    event['FileName'] = ''\n\n                # Lookup geolocation info for the IP (if possible)\n                ip = event['ClientIP']\n                if geo_reader and ip != 'N/A':\n                    try:\n                        # Handle IPv6 addresses that might have port info\n                        if ':' in ip and not ip.startswith('['):\n                            ip = ip.split(':')[0]\n                            \n                        response = geo_reader.city(ip)\n                        \n                        country = response.country.iso_code if response.country.iso_code else \"Unknown\"\n                        city = response.city.name if response.city.name else \"Unknown\"\n                        region = response.subdivisions.most_specific.name if response.subdivisions.most_specific.name else \"Unknown\"\n                        \n                        event['Country'] = country\n                        event['Region'] = region\n                        event['City'] = city\n                        \n                        # Add coordinates for mapping\n                        event['Latitude'] = response.location.latitude\n                        event['Longitude'] = response.location.longitude\n                        \n                    except Exception as e:\n                        event['Country'] = \"Unknown\"\n                        event['Region'] = \"Unknown\"\n                        event['City'] = \"Unknown\"\n                        event['Latitude'] = None\n                        event['Longitude'] = None\n                else:\n                    event['Country'] = \"Unknown\"\n                    event['Region'] = \"Unknown\"\n                    event['City'] = \"Unknown\"\n                    event['Latitude'] = None\n                    event['Longitude'] = None\n\n                timeline.append(event)\n            except json.JSONDecodeError:\n                error_count += 1\n        \n        if geo_reader:\n            geo_reader.close()\n\n        if len(timeline) == 0:\n            return None, \"No valid data could be parsed from the file\"\n            \n        # Provide a warning if there were parsing errors\n        warning = None\n        if error_count > 0:\n            warning = f\"{error_count} records had invalid JSON data and were skipped.\"\n            \n        return timeline, warning\n\n    except Exception as e:\n        return None, f\"Error processing file: {str(e)}\"\n\ndef is_ip_anomalous(event):\n    \"\"\"\n    Returns True if the event's IP is considered anomalous.\n    An IP is NOT anomalous if it is exactly '192.168.1.160' (Frans usual IP).\n    Otherwise, if the geolocated Country is not 'US' or the Region is not 'Massachusetts',\n    then the event is considered anomalous.\n    \"\"\"\n    ip = event.get('ClientIP', 'N/A')\n    if ip == '192.168.1.160':\n        return False\n    country = event.get('Country', \"Unknown\")\n    region = event.get('Region', \"Unknown\")\n    # If the country is not usa or region is not Mass, flag as anomalous\n    if country != 'US' or region != 'Massachusetts':\n        return True\n    return False\n\ndef detect_compromised_events(timeline):\n    \"\"\"\n    Returns events exhibiting suspicious activity AND having an anomalous IP.\n    Suspicious activity is defined as:\n      - Operation is either 'SoftDelete' or 'MoveToDeletedItems'\n      OR\n      - The user has logged in from more than 3 unique IP addresses.\n    \"\"\"\n    suspicious_ops = ['SoftDelete', 'MoveToDeletedItems']\n    compromised_events = []\n    user_ip_map = {}\n\n    # First, build a map of unique IP addresses\n    for event in timeline:\n        user_id = event['UserId']\n        client_ip = event['ClientIP']\n        if user_id not in user_ip_map:\n            user_ip_map[user_id] = set()\n        user_ip_map[user_id].add(client_ip)\n\n    # Flag an event as compromised if it looks like suspicious activity and has a weird IP\n    for event in timeline:\n        user_id = event['UserId']\n        suspicious_activity = (event['Operation'] in suspicious_ops or len(user_ip_map[user_id]) > 3)\n        if suspicious_activity and is_ip_anomalous(event):\n            compromised_events.append(event)\n    \n    return compromised_events\n\ndef filter_files_accessed(timeline):\n    \"\"\"Return events where the operation is 'FileAccessed'.\"\"\"\n    return [event for event in timeline if event['Operation'] == 'FileAccessed']\n\ndef filter_anomalous_ips(timeline):\n    \"\"\"Return only events where the IP is considered anomalous per our criteria.\"\"\"\n    return [event for event in timeline if is_ip_anomalous(event)]\n\ndef get_stats(timeline, compromised_events, files_accessed, anomalous_ips):\n    \"\"\"Generate statistics about the data.\"\"\"\n    stats = {\n        \"total_events\": len(timeline),\n        \"compromised_events\": len(compromised_events),\n        \"files_accessed\": len(files_accessed),\n        \"anomalous_ips\": len(anomalous_ips),\n        \"unique_users\": len(set(event['UserId'] for event in timeline)),\n        \"unique_ips\": len(set(event['ClientIP'] for event in timeline)),\n        \"unique_operations\": len(set(event['Operation'] for event in timeline)),\n        \"operation_counts\": {},\n        \"country_counts\": {},\n    }\n    \n    # Count operations\n    for event in timeline:\n        op = event['Operation']\n        stats[\"operation_counts\"][op] = stats[\"operation_counts\"].get(op, 0) + 1\n    \n    # Count countries\n    for event in timeline:\n        country = event.get('Country', 'Unknown')\n        stats[\"country_counts\"][country] = stats[\"country_counts\"].get(country, 0) + 1\n    \n    return stats\n\ndef get_ip_summary(timeline):\n    \"\"\"Generate a summary of IP addresses in the dataset.\"\"\"\n    ip_summary = {}\n    \n    for event in timeline:\n        ip = event.get('ClientIP', 'N/A')\n        if ip == 'N/A':\n            continue\n            \n        if ip not in ip_summary:\n            ip_summary[ip] = {\n                'count': 0,\n                'country': event.get('Country', 'Unknown'),\n                'region': event.get('Region', 'Unknown'),\n                'city': event.get('City', 'Unknown'),\n                'latitude': event.get('Latitude'),\n                'longitude': event.get('Longitude'),\n                'operations': set(),\n                'users': set(),\n                'is_anomalous': is_ip_anomalous(event),\n                'first_seen': event['Timestamp'],\n                'last_seen': event['Timestamp']\n            }\n        \n        ip_summary[ip]['count'] += 1\n        ip_summary[ip]['operations'].add(event['Operation'])\n        ip_summary[ip]['users'].add(event['UserId'])\n        \n        # Update first/last seen\n        if event['Timestamp'] < ip_summary[ip]['first_seen']:\n            ip_summary[ip]['first_seen'] = event['Timestamp']\n        if event['Timestamp'] > ip_summary[ip]['last_seen']:\n            ip_summary[ip]['last_seen'] = event['Timestamp']\n    \n    # Convert sets to lists for easier display\n    for ip in ip_summary:\n        ip_summary[ip]['operations'] = list(ip_summary[ip]['operations'])\n        ip_summary[ip]['users'] = list(ip_summary[ip]['users'])\n    \n    return ip_summary\n","size_bytes":9376},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"folium>=0.19.5\",\n    \"geoip2>=5.0.1\",\n    \"openpyxl>=3.1.5\",\n    \"pandas>=2.2.3\",\n    \"streamlit-folium>=0.25.0\",\n    \"streamlit>=1.44.1\",\n    \"trafilatura>=2.0.0\",\n]\n","size_bytes":314},"replit.md":{"content":"# Overview\n\n**Microsoft Purview Security Analyzer** - A comprehensive security audit log analysis tool built with Streamlit. This open-source application helps security professionals identify anomalous IP activities and potential security breaches in Microsoft Purview audit logs. The tool processes CSV and Excel files, performs geolocation analysis, and visualizes suspicious activities on an interactive dark-themed interface with world mapping capabilities.\n\n## Project Status\n- **Current Phase**: Production-ready for GitHub release\n- **Name**: Microsoft Purview Security Analyzer  \n- **Purpose**: Open-source tool for security community\n- **UI Theme**: Dark mode for security analyst preference\n- **Session Management**: Optimized to prevent map interaction reloading issues\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Frontend Architecture\nThe application uses Streamlit as the web framework, providing a simple and intuitive interface for security analysts. The UI is organized with a sidebar for file uploads and configuration, and a main content area for displaying analysis results, maps, and data tables. The architecture supports real-time processing and visualization of uploaded audit log files.\n\n## Data Processing Pipeline\nThe core processing logic is distributed across multiple modules:\n- **File Processing**: Handles CSV and Excel file parsing with robust error handling and encoding detection\n- **Security Analysis**: Performs IP geolocation, anomaly detection, and event classification\n- **Visualization**: Creates interactive maps using Folium and generates charts for data insights\n\n## Geolocation Services\nThe system integrates with the GeoLite2 database for IP geolocation services. It includes fallback mechanisms for known private IPs and handles cases where geolocation data is unavailable. The architecture supports both online and offline geolocation analysis.\n\n## Data Analysis Engine\nThe application implements several analysis algorithms:\n- **Anomalous IP Detection**: Uses statistical methods to identify IPs with unusual access patterns\n- **Compromised Event Identification**: Analyzes audit logs for suspicious activities and security breaches\n- **File Access Monitoring**: Tracks and filters file access events for security analysis\n- **Timeline Analysis**: Processes chronological event data for pattern recognition\n\n## Session Management\nThe application uses Streamlit's session state to maintain processed data between user interactions, preventing unnecessary reprocessing of large datasets and improving performance.\n\n# External Dependencies\n\n## Geolocation Database\n- **GeoLite2-City.mmdb**: MaxMind's GeoLite2 database for IP geolocation services\n- Optional upload functionality allows users to provide their own GeoIP database\n\n## Python Libraries\n- **Streamlit**: Web application framework and UI components\n- **Pandas**: Data manipulation and analysis for audit log processing\n- **Folium**: Interactive map generation and geospatial visualization\n- **Streamlit-Folium**: Integration layer between Streamlit and Folium maps\n- **GeoIP2**: Python library for interfacing with MaxMind GeoIP databases\n- **Plotly**: Additional charting and visualization capabilities (referenced in visualizer.py)\n\n## File Format Support\n- **CSV Files**: Standard comma-separated values with multiple encoding support\n- **Excel Files**: .xlsx and .xls formats through pandas Excel engine\n\n## Map Services\n- **OpenStreetMap**: Default tile provider for map visualization\n- **Folium Plugins**: MarkerCluster for performance optimization with large datasets","size_bytes":3642},"security_analyzer.py":{"content":"import pandas as pd\nimport json\nimport os\nimport time\nimport ipaddress\nimport geoip2.database\n\n# Pre-defined IP location data for known IPs (like Frans' usual IP)\nKNOWN_IPS = {\n    '192.168.1.160': {\n        'Country': 'US',\n        'Region': 'Massachusetts',\n        'City': 'Boston',\n        'Latitude': 42.3601,\n        'Longitude': -71.0589\n    }\n}\n\ndef is_private_ip(ip):\n    \"\"\"Check if an IP address is private.\"\"\"\n    if ip == 'N/A':\n        return True\n    try:\n        return ipaddress.ip_address(ip).is_private\n    except ValueError:\n        return True  # If not a valid IP, treat as private\n\ndef get_ip_geolocation(ip, geo_reader=None):\n    \"\"\"\n    Get geolocation data for an IP address using GeoLite2 database.\n    \n    Parameters:\n    ip (str): IP address\n    geo_reader: GeoLite2 database reader object\n    \n    Returns:\n    dict: Geolocation data (country, region, city, latitude, longitude)\n    \"\"\"\n    # Check for known IPs first (like Frans's usual IP)\n    if ip in KNOWN_IPS:\n        return KNOWN_IPS[ip]\n    \n    # Skip private IPs\n    if is_private_ip(ip):\n        return {\n            'Country': 'Local',\n            'Region': 'Network',\n            'City': 'Private',\n            'Latitude': 0,\n            'Longitude': 0\n        }\n    \n    # Try to use the GeoLite2 database\n    if geo_reader and ip != 'N/A':\n        try:\n            response = geo_reader.city(ip)\n            country = response.country.iso_code if response.country.iso_code else \"Unknown\"\n            region = response.subdivisions.most_specific.name if response.subdivisions.most_specific.name else \"Unknown\"\n            city = response.city.name if response.city.name else \"Unknown\"\n            latitude = response.location.latitude if response.location.latitude else 0\n            longitude = response.location.longitude if response.location.longitude else 0\n            \n            return {\n                'Country': country,\n                'Region': region,\n                'City': city,\n                'Latitude': latitude,\n                'Longitude': longitude\n            }\n        except Exception as e:\n            print(f\"Error looking up IP {ip} in GeoLite2 database: {e}\")\n    \n    # Default values if all lookups fail\n    return {\n        'Country': 'Unknown',\n        'Region': 'Unknown',\n        'City': 'Unknown',\n        'Latitude': 0,\n        'Longitude': 0\n    }\n\ndef parse_audit_logs(file_path, geoip_db_path=\"./attached_assets/GeoLite2-City.mmdb\"):\n    \"\"\"\n    Parse audit logs from CSV or Excel file and return a timeline of events with geolocation data.\n    \n    Parameters:\n    file_path (str): Path to the input CSV or Excel file\n    geoip_db_path (str): Path to the GeoLite2 database\n    \n    Returns:\n    list: Timeline of events with geolocation data\n    \"\"\"\n    try:\n        # Initialize GeoIP2 reader if database file exists\n        geo_reader = None\n        try:\n            if os.path.exists(geoip_db_path):\n                geo_reader = geoip2.database.Reader(geoip_db_path)\n                print(f\"Successfully loaded GeoLite2 database from {geoip_db_path}\")\n            else:\n                print(f\"GeoLite2 database not found at {geoip_db_path}. Trying other locations...\")\n                # Try alternate locations\n                alt_paths = [\n                    \"GeoLite2-City.mmdb\",\n                    \"./GeoLite2-City.mmdb\",\n                    \"../attached_assets/GeoLite2-City.mmdb\",\n                    \"/attached_assets/GeoLite2-City.mmdb\",\n                ]\n                for alt_path in alt_paths:\n                    if os.path.exists(alt_path):\n                        geo_reader = geoip2.database.Reader(alt_path)\n                        print(f\"Successfully loaded GeoLite2 database from {alt_path}\")\n                        break\n                \n                if not geo_reader:\n                    print(\"GeoLite2 database not found in any location. IP geolocation will be limited.\")\n        except Exception as e:\n            print(f\"Error opening GeoIP database: {e}\")\n            geo_reader = None\n\n        # Determine file type and read accordingly\n        file_extension = os.path.splitext(file_path)[1].lower()\n        \n        if file_extension == '.csv':\n            df = pd.read_csv(file_path)\n        elif file_extension in ['.xlsx', '.xls']:\n            df = pd.read_excel(file_path)\n        else:\n            raise ValueError(f\"Unsupported file format: {file_extension}\")\n\n        # Select relevant columns if they exist\n        columns_of_interest = ['CreationDate', 'Operation', 'UserId', 'AuditData']\n        for col in columns_of_interest:\n            if col not in df.columns:\n                raise ValueError(f\"Required column '{col}' not found in the input file\")\n        \n        df = df[columns_of_interest]\n\n        # Convert CreationDate to datetime for sorting\n        df['CreationDate'] = pd.to_datetime(df['CreationDate'])\n        df = df.sort_values(by='CreationDate')\n\n        # Parse AuditData and create a timeline with geolocation data\n        timeline = []\n        # Keep track of IPs we've already looked up to avoid repeated lookups\n        ip_geo_cache = {}\n        \n        for _, row in df.iterrows():\n            try:\n                # Check if AuditData is a string, if so, parse it\n                audit_data = row['AuditData']\n                if isinstance(audit_data, str):\n                    audit_data = json.loads(audit_data)\n                elif not isinstance(audit_data, dict):\n                    # If it's neither a string nor a dict, skip this row\n                    continue\n                \n                event = {\n                    'Timestamp': row['CreationDate'],\n                    'Operation': row['Operation'],\n                    'UserId': row['UserId'],\n                    'ClientIP': audit_data.get('ClientIP', 'N/A'),\n                    'ResultStatus': audit_data.get('ResultStatus', 'N/A')\n                }\n                \n                # For FileAccessed events, extract the file name if available\n                if row['Operation'] == 'FileAccessed':\n                    event['FileName'] = audit_data.get('SourceFileName', 'N/A')\n                else:\n                    event['FileName'] = ''\n\n                # Lookup geolocation info for the IP (if possible)\n                ip = event['ClientIP']\n                \n                # Use our cache to avoid looking up the same IP multiple times\n                if ip in ip_geo_cache:\n                    event.update(ip_geo_cache[ip])\n                else:\n                    # Get geolocation data and cache it\n                    geo_data = get_ip_geolocation(ip, geo_reader)\n                    ip_geo_cache[ip] = geo_data\n                    event.update(geo_data)\n\n                timeline.append(event)\n            except json.JSONDecodeError:\n                print(f\"Error parsing AuditData for record at {row['CreationDate']}\")\n        \n        # Clean up geo reader\n        if geo_reader:\n            geo_reader.close()\n            \n        return timeline\n\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        raise\n\ndef is_ip_anomalous(event):\n    \"\"\"\n    Returns True if the event's IP is considered anomalous.\n    An IP is NOT anomalous if it is exactly '192.168.1.160' (Frans usual IP).\n    Otherwise, if the geolocated Country is not 'US' or the Region is not 'Massachusetts',\n    then the event is considered anomalous.\n    \"\"\"\n    ip = event.get('ClientIP', 'N/A')\n    if ip == '192.168.1.160':\n        return False\n    country = event.get('Country', \"Unknown\")\n    region = event.get('Region', \"Unknown\")\n    # If the country is not USA or region is not Mass, flag as anomalous\n    if country != 'US' or region != 'Massachusetts':\n        return True\n    return False\n\ndef detect_compromised_events(timeline):\n    \"\"\"\n    Returns events exhibiting suspicious activity AND having an anomalous IP.\n    Suspicious activity is defined as:\n      - Operation is either 'SoftDelete' or 'MoveToDeletedItems'\n      OR\n      - The user has logged in from more than 3 unique IP addresses.\n    \"\"\"\n    suspicious_ops = ['SoftDelete', 'MoveToDeletedItems']\n    compromised_events = []\n    user_ip_map = {}\n\n    # First, build a map of unique IP addresses\n    for event in timeline:\n        user_id = event['UserId']\n        client_ip = event['ClientIP']\n        if user_id not in user_ip_map:\n            user_ip_map[user_id] = set()\n        user_ip_map[user_id].add(client_ip)\n\n    # Flag an event as compromised if it looks like suspicious activity and has a weird IP\n    for event in timeline:\n        user_id = event['UserId']\n        suspicious_activity = (event['Operation'] in suspicious_ops or len(user_ip_map[user_id]) > 3)\n        if suspicious_activity and is_ip_anomalous(event):\n            compromised_events.append(event)\n    return compromised_events\n\ndef filter_files_accessed(timeline):\n    \"\"\"Return events where the operation is 'FileAccessed'.\"\"\"\n    return [event for event in timeline if event['Operation'] == 'FileAccessed']\n\ndef filter_anomalous_ips(timeline):\n    \"\"\"Return only events where the IP is considered anomalous per our criteria.\"\"\"\n    return [event for event in timeline if is_ip_anomalous(event)]","size_bytes":9265},"utils.py":{"content":"import pandas as pd\nimport json\nimport geoip2.database\nimport os\n\ndef parse_audit_logs(file_path, geoip_db_path=\"GeoLite2-City.mmdb\"):\n    \"\"\"\n    Parse audit logs from CSV or Excel file and return a timeline of events with geolocation data.\n    \n    Parameters:\n    file_path (str): Path to the input CSV or Excel file\n    geoip_db_path (str): Path to the GeoLite2-City.mmdb database file\n    \n    Returns:\n    list: Timeline of events with geolocation data\n    \"\"\"\n    try:\n        # Determine file type and read accordingly\n        file_extension = os.path.splitext(file_path)[1].lower()\n        \n        if file_extension == '.csv':\n            df = pd.read_csv(file_path)\n        elif file_extension in ['.xlsx', '.xls']:\n            df = pd.read_excel(file_path)\n        else:\n            raise ValueError(f\"Unsupported file format: {file_extension}\")\n\n        # Select relevant columns if they exist\n        columns_of_interest = ['CreationDate', 'Operation', 'UserId', 'AuditData']\n        for col in columns_of_interest:\n            if col not in df.columns:\n                raise ValueError(f\"Required column '{col}' not found in the input file\")\n        \n        df = df[columns_of_interest]\n\n        # Convert CreationDate to datetime for sorting\n        df['CreationDate'] = pd.to_datetime(df['CreationDate'])\n        df = df.sort_values(by='CreationDate')\n\n        # Open the GeoLite2 database\n        try:\n            geo_reader = geoip2.database.Reader(geoip_db_path) if geoip_db_path else None\n        except Exception as ge:\n            print(f\"Error opening GeoIP database: {ge}\")\n            geo_reader = None\n\n        # Parse AuditData and create a timeline with geolocation data\n        timeline = []\n        for _, row in df.iterrows():\n            try:\n                # Check if AuditData is a string, if so, parse it\n                audit_data = row['AuditData']\n                if isinstance(audit_data, str):\n                    audit_data = json.loads(audit_data)\n                elif not isinstance(audit_data, dict):\n                    # If it's neither a string nor a dict, skip this row\n                    continue\n                \n                event = {\n                    'Timestamp': row['CreationDate'],\n                    'Operation': row['Operation'],\n                    'UserId': row['UserId'],\n                    'ClientIP': audit_data.get('ClientIP', 'N/A'),\n                    'ResultStatus': audit_data.get('ResultStatus', 'N/A')\n                }\n                \n                # For FileAccessed events, extract the file name if available\n                if row['Operation'] == 'FileAccessed':\n                    event['FileName'] = audit_data.get('SourceFileName', 'N/A')\n                else:\n                    event['FileName'] = ''\n\n                # Lookup geolocation info for the IP (if possible)\n                ip = event['ClientIP']\n                if geo_reader and ip != 'N/A':\n                    try:\n                        response = geo_reader.city(ip)\n                        country = response.country.iso_code if response.country.iso_code else \"Unknown\"\n                        region = response.subdivisions.most_specific.name if response.subdivisions.most_specific.name else \"Unknown\"\n                        city = response.city.name if response.city.name else \"Unknown\"\n                        latitude = response.location.latitude if response.location.latitude else 0\n                        longitude = response.location.longitude if response.location.longitude else 0\n                        \n                        event['Country'] = country\n                        event['Region'] = region\n                        event['City'] = city\n                        event['Latitude'] = latitude\n                        event['Longitude'] = longitude\n                    except Exception as e:\n                        event['Country'] = \"Unknown\"\n                        event['Region'] = \"Unknown\"\n                        event['City'] = \"Unknown\"\n                        event['Latitude'] = 0\n                        event['Longitude'] = 0\n                else:\n                    event['Country'] = \"Unknown\"\n                    event['Region'] = \"Unknown\"\n                    event['City'] = \"Unknown\"\n                    event['Latitude'] = 0\n                    event['Longitude'] = 0\n\n                timeline.append(event)\n            except json.JSONDecodeError:\n                print(f\"Error parsing AuditData for record at {row['CreationDate']}\")\n        \n        if geo_reader:\n            geo_reader.close()\n\n        return timeline\n\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        raise\n\ndef is_ip_anomalous(event):\n    \"\"\"\n    Returns True if the event's IP is considered anomalous.\n    An IP is NOT anomalous if it is exactly '192.168.1.160' (Frans usual IP).\n    Otherwise, if the geolocated Country is not 'US' or the Region is not 'Massachusetts',\n    then the event is considered anomalous.\n    \"\"\"\n    ip = event.get('ClientIP', 'N/A')\n    if ip == '192.168.1.160':\n        return False\n    country = event.get('Country', \"Unknown\")\n    region = event.get('Region', \"Unknown\")\n    # If the country is not USA or region is not Mass, flag as anomalous\n    if country != 'US' or region != 'Massachusetts':\n        return True\n    return False\n\ndef detect_compromised_events(timeline):\n    \"\"\"\n    Returns events exhibiting suspicious activity AND having an anomalous IP.\n    Suspicious activity is defined as:\n      - Operation is either 'SoftDelete' or 'MoveToDeletedItems'\n      OR\n      - The user has logged in from more than 3 unique IP addresses.\n    \"\"\"\n    suspicious_ops = ['SoftDelete', 'MoveToDeletedItems']\n    compromised_events = []\n    user_ip_map = {}\n\n    # First, build a map of unique IP addresses\n    for event in timeline:\n        user_id = event['UserId']\n        client_ip = event['ClientIP']\n        if user_id not in user_ip_map:\n            user_ip_map[user_id] = set()\n        user_ip_map[user_id].add(client_ip)\n\n    # Flag an event as compromised if it looks like suspicious activity and has a weird IP\n    for event in timeline:\n        user_id = event['UserId']\n        suspicious_activity = (event['Operation'] in suspicious_ops or len(user_ip_map[user_id]) > 3)\n        if suspicious_activity and is_ip_anomalous(event):\n            compromised_events.append(event)\n    return compromised_events\n\ndef filter_files_accessed(timeline):\n    \"\"\"Return events where the operation is 'FileAccessed'.\"\"\"\n    return [event for event in timeline if event['Operation'] == 'FileAccessed']\n\ndef filter_anomalous_ips(timeline):\n    \"\"\"Return only events where the IP is considered anomalous per our criteria.\"\"\"\n    return [event for event in timeline if is_ip_anomalous(event)]\n\n","size_bytes":6834},"visualizer.py":{"content":"import folium\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom folium.plugins import MarkerCluster\nimport streamlit as st\n\ndef create_ip_map(ip_events):\n    \"\"\"\n    Creates a folium map with markers for IP addresses.\n    \n    Args:\n        ip_events: List of events with IP data, including lat/long\n    \n    Returns:\n        A folium map object\n    \"\"\"\n    # Create base map centered at a default location\n    m = folium.Map(location=[20, 0], zoom_start=2, )\n    \n    # Add marker clusters to handle many points\n    marker_cluster = MarkerCluster().add_to(m)\n    \n    # Track IPs that have been added to avoid duplicates\n    added_ips = set()\n    \n    # Add markers for each IP\n    for event in ip_events:\n        ip = event.get('ClientIP')\n        lat = event.get('Latitude')\n        lon = event.get('Longitude')\n        \n        # Skip if no location data or already added\n        if not lat or not lon or ip in added_ips:\n            continue\n            \n        added_ips.add(ip)\n        \n        # Determine if anomalous\n        is_anomalous = is_ip_anomalous(event)\n        \n        # Create popup content\n        popup_html = f\"\"\"\n        <div style=\"width: 200px\">\n            <b>IP:</b> {ip}<br>\n            <b>Country:</b> {event.get('Country', 'Unknown')}<br>\n            <b>Region:</b> {event.get('Region', 'Unknown')}<br>\n            <b>City:</b> {event.get('City', 'Unknown')}<br>\n            <b>User:</b> {event.get('UserId', 'Unknown')}<br>\n            <b>First Seen:</b> {event.get('Timestamp').strftime('%Y-%m-%d %H:%M:%S')}<br>\n            <b>Anomalous:</b> {\"Yes\" if is_anomalous else \"No\"}<br>\n        </div>\n        \"\"\"\n        \n        # Choose color based on anomalous status\n        color = 'red' if is_anomalous else 'blue'\n        \n        # Add marker to the cluster\n        folium.Marker(\n            location=[lat, lon],\n            popup=folium.Popup(popup_html, max_width=300),\n            icon=folium.Icon(color=color, icon='info-sign'),\n        ).add_to(marker_cluster)\n    \n    return m\n\ndef is_ip_anomalous(event):\n    \"\"\"\n    Returns True if the event's IP is considered anomalous.\n    An IP is NOT anomalous if it is exactly '192.168.1.160' (Frans usual IP).\n    Otherwise, if the geolocated Country is not 'US' or the Region is not 'Massachusetts',\n    then the event is considered anomalous.\n    \"\"\"\n    ip = event.get('ClientIP', 'N/A')\n    if ip == '192.168.1.160':\n        return False\n    country = event.get('Country', \"Unknown\")\n    region = event.get('Region', \"Unknown\")\n    # If the country is not usa or region is not Mass, flag as anomalous\n    if country != 'US' or region != 'Massachusetts':\n        return True\n    return False\n\ndef create_ip_detail_map(ip_data):\n    \"\"\"Creates a map for a single IP with detailed information.\"\"\"\n    if not ip_data.get('latitude') or not ip_data.get('longitude'):\n        return None\n        \n    m = folium.Map(location=[ip_data['latitude'], ip_data['longitude']], zoom_start=10)\n    \n    # Create popup with detailed information\n    popup_html = f\"\"\"\n    <div style=\"width: 250px\">\n        <h4>{ip_data.get('ip', 'Unknown IP')}</h4>\n        <b>Location:</b> {ip_data.get('city', 'Unknown')}, {ip_data.get('region', 'Unknown')}, {ip_data.get('country', 'Unknown')}<br>\n        <b>Events:</b> {ip_data.get('count', 0)}<br>\n        <b>Users:</b> {', '.join(ip_data.get('users', []))}<br>\n        <b>First Seen:</b> {ip_data.get('first_seen').strftime('%Y-%m-%d %H:%M:%S') if isinstance(ip_data.get('first_seen'), pd.Timestamp) else 'Unknown'}<br>\n        <b>Last Seen:</b> {ip_data.get('last_seen').strftime('%Y-%m-%d %H:%M:%S') if isinstance(ip_data.get('last_seen'), pd.Timestamp) else 'Unknown'}<br>\n        <b>Anomalous:</b> {\"Yes\" if ip_data.get('is_anomalous', False) else \"No\"}<br>\n    </div>\n    \"\"\"\n    \n    # Add marker\n    color = 'red' if ip_data.get('is_anomalous', False) else 'blue'\n    folium.Marker(\n        location=[ip_data['latitude'], ip_data['longitude']],\n        popup=folium.Popup(popup_html, max_width=300),\n        icon=folium.Icon(color=color, icon='info-sign'),\n    ).add_to(m)\n    \n    return m\n\ndef create_operations_chart(timeline):\n    \"\"\"Create a bar chart of operations.\"\"\"\n    operations = {}\n    for event in timeline:\n        op = event.get('Operation', 'Unknown')\n        operations[op] = operations.get(op, 0) + 1\n    \n    # Convert to dataframe\n    df = pd.DataFrame({\n        'Operation': list(operations.keys()),\n        'Count': list(operations.values())\n    })\n    \n    # Sort by count descending\n    df = df.sort_values('Count', ascending=False)\n    \n    # Create chart\n    fig = px.bar(\n        df,\n        x='Operation',\n        y='Count',\n        title='Operations by Frequency',\n        color='Operation'\n    )\n    \n    return fig\n\ndef create_country_chart(timeline):\n    \"\"\"Create a bar chart of countries.\"\"\"\n    countries = {}\n    for event in timeline:\n        country = event.get('Country', 'Unknown')\n        countries[country] = countries.get(country, 0) + 1\n    \n    # Convert to dataframe\n    df = pd.DataFrame({\n        'Country': list(countries.keys()),\n        'Count': list(countries.values())\n    })\n    \n    # Sort by count descending\n    df = df.sort_values('Count', ascending=False)\n    \n    # Create chart\n    fig = px.bar(\n        df,\n        x='Country',\n        y='Count',\n        title='Events by Country',\n        color='Country'\n    )\n    \n    return fig\n\ndef create_timeline_chart(timeline):\n    \"\"\"Create a timeline chart showing events over time.\"\"\"\n    # Convert to dataframe\n    df = pd.DataFrame(timeline)\n    \n    # Group by day and operation\n    df['date'] = df['Timestamp'].dt.date\n    daily_counts = df.groupby(['date', 'Operation']).size().reset_index(name='count')\n    \n    # Create chart\n    fig = px.line(\n        daily_counts,\n        x='date',\n        y='count',\n        color='Operation',\n        title='Events Over Time',\n        labels={'date': 'Date', 'count': 'Number of Events'}\n    )\n    \n    return fig\n\ndef create_anomalous_ip_chart(timeline):\n    \"\"\"Create a pie chart showing anomalous vs normal IPs.\"\"\"\n    # Count anomalous vs normal\n    anomalous = 0\n    normal = 0\n    \n    for event in timeline:\n        if is_ip_anomalous(event):\n            anomalous += 1\n        else:\n            normal += 1\n    \n    # Create chart\n    fig = go.Figure(data=[go.Pie(\n        labels=['Anomalous', 'Normal'],\n        values=[anomalous, normal],\n        hole=.3,\n        marker_colors=['#FF5252', '#4CAF50']\n    )])\n    \n    fig.update_layout(title_text=\"Anomalous vs Normal Events\")\n    \n    return fig\n","size_bytes":6621},"utils/__init__.py":{"content":"# This file ensures the utils directory is treated as a package\n","size_bytes":64},"utils/file_processor.py":{"content":"import pandas as pd\nimport os\n\ndef process_file(file_path):\n    \"\"\"\n    Process the uploaded CSV or Excel file and return a pandas DataFrame.\n    \n    Args:\n        file_path (str): Path to the uploaded file\n        \n    Returns:\n        pandas.DataFrame: Processed data from the file\n    \"\"\"\n    # Determine file type based on extension\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Process CSV file\n    if file_extension == '.csv':\n        try:\n            # Try different encodings and delimiters\n            try:\n                df = pd.read_csv(file_path, encoding='utf-8')\n            except UnicodeDecodeError:\n                df = pd.read_csv(file_path, encoding='latin1')\n            except pd.errors.ParserError:\n                # Try with different delimiter\n                df = pd.read_csv(file_path, encoding='utf-8', delimiter=';')\n        except Exception as e:\n            raise Exception(f\"Failed to process CSV file: {str(e)}\")\n    \n    # Process Excel file\n    elif file_extension in ['.xlsx', '.xls']:\n        try:\n            df = pd.read_excel(file_path)\n        except Exception as e:\n            raise Exception(f\"Failed to process Excel file: {str(e)}\")\n    \n    else:\n        raise ValueError(f\"Unsupported file format: {file_extension}\")\n    \n    # Basic data cleaning\n    # Remove duplicate rows\n    df = df.drop_duplicates()\n    \n    # Handle missing values\n    df = df.fillna('')\n    \n    # Standardize column names (lowercase, replace spaces with underscores)\n    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n    \n    # Identify IP address columns\n    ip_columns = [col for col in df.columns if any(term in col for term in ['ip', 'address', 'source'])]\n    \n    # If no IP columns found, raise an error\n    if not ip_columns:\n        raise ValueError(\"No IP address columns found in the file. Please ensure your data contains IP information.\")\n    \n    # Ensure at least one column contains valid IPs\n    valid_ip_found = False\n    for col in ip_columns:\n        # Check if any value in the column matches basic IP format\n        if df[col].astype(str).str.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$').any():\n            valid_ip_found = True\n            break\n    \n    if not valid_ip_found:\n        raise ValueError(\"No valid IP addresses found in the expected columns.\")\n    \n    return df\n","size_bytes":2374},"utils/ip_analyzer.py":{"content":"import pandas as pd\nimport re\nimport geoip2.database\nimport os\nfrom pathlib import Path\n\ndef analyze_ips(df):\n    \"\"\"\n    Analyze the DataFrame to identify anomalous IPs and their details.\n    \n    Args:\n        df (pandas.DataFrame): DataFrame containing the log data\n        \n    Returns:\n        tuple: (list of anomalous IPs, dictionary with IP details)\n    \"\"\"\n    # Identify IP address columns\n    ip_columns = [col for col in df.columns if any(term in col for term in ['ip', 'address', 'source'])]\n    \n    # If no IP columns found, try to find columns with IP-like values\n    if not ip_columns:\n        for col in df.columns:\n            # Check if any value in the column matches basic IP format\n            if df[col].astype(str).str.match(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$').any():\n                ip_columns.append(col)\n    \n    # If still no IP columns found, raise an error\n    if not ip_columns:\n        raise ValueError(\"No IP address columns found in the file. Please ensure your data contains IP information.\")\n    \n    # Identify file/resource access columns\n    file_columns = [col for col in df.columns if any(term in col for term in ['file', 'resource', 'path', 'access'])]\n    \n    # Use the first IP column if multiple are found\n    ip_column = ip_columns[0]\n    \n    # Extract all unique IPs\n    all_ips = pd.unique(df[ip_column].astype(str))\n    \n    # Filter for valid IPs\n    ip_pattern = re.compile(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$')\n    valid_ips = [ip for ip in all_ips if ip_pattern.match(ip)]\n    \n    # Identify anomalous IPs (this is a simple heuristic that can be improved)\n    # For this example, we'll consider IPs anomalous if they appear less frequently\n    ip_counts = df[ip_column].value_counts()\n    \n    # Calculate average and standard deviation of access counts\n    avg_count = ip_counts.mean()\n    std_count = ip_counts.std()\n    \n    # IPs with very low frequency (less than average - 1 standard deviation)\n    # or very high frequency (more than average + 2 standard deviations) are considered anomalous\n    anomalous_ips = []\n    for ip in valid_ips:\n        count = ip_counts.get(ip, 0)\n        if count < max(1, avg_count - std_count) or count > avg_count + 2 * std_count:\n            anomalous_ips.append(ip)\n    \n    # Get details for each anomalous IP\n    ip_details = {}\n    for ip in anomalous_ips:\n        # Get accessed files for this IP\n        if file_columns:\n            file_col = file_columns[0]\n            accessed_files = df[df[ip_column] == ip][file_col].unique().tolist()\n        else:\n            accessed_files = []\n        \n        # Get geolocation data\n        geo_data = get_ip_geolocation(ip)\n        \n        # Store all details\n        ip_details[ip] = {\n            'country': geo_data.get('country', 'Unknown'),\n            'city': geo_data.get('city', 'Unknown'),\n            'latitude': geo_data.get('latitude', 0),\n            'longitude': geo_data.get('longitude', 0),\n            'accessed_files': accessed_files,\n            'access_count': int(ip_counts.get(ip, 0))\n        }\n    \n    return anomalous_ips, ip_details\n\ndef get_ip_geolocation(ip):\n    \"\"\"\n    Get geolocation data for an IP address using GeoLite2-City.mmdb.\n    \n    Args:\n        ip (str): IP address\n        \n    Returns:\n        dict: Geolocation data (country, city, latitude, longitude)\n    \"\"\"\n    # Try common paths for the GeoLite2-City.mmdb database\n    possible_paths = [\n        './GeoLite2-City.mmdb',\n        '../GeoLite2-City.mmdb',\n        '/app/GeoLite2-City.mmdb',\n        str(Path.home() / 'GeoLite2-City.mmdb')\n    ]\n    \n    db_path = None\n    for path in possible_paths:\n        if os.path.exists(path):\n            db_path = path\n            break\n    \n    if not db_path:\n        # If database file not found, return dummy data\n        return {\n            'country': 'Unknown',\n            'city': 'Unknown',\n            'latitude': 0,\n            'longitude': 0\n        }\n    \n    try:\n        # Open the GeoLite2 database\n        with geoip2.database.Reader(db_path) as reader:\n            # Look up the IP address\n            response = reader.city(ip)\n            \n            # Extract relevant information\n            return {\n                'country': response.country.name or 'Unknown',\n                'city': response.city.name or 'Unknown',\n                'latitude': response.location.latitude or 0,\n                'longitude': response.location.longitude or 0\n            }\n    except Exception as e:\n        # Handle errors (invalid IP, not found in database, etc.)\n        print(f\"Error getting geolocation for IP {ip}: {str(e)}\")\n        return {\n            'country': 'Unknown',\n            'city': 'Unknown',\n            'latitude': 0,\n            'longitude': 0\n        }\n","size_bytes":4784},"utils/map_generator.py":{"content":"import folium\nfrom folium.plugins import MarkerCluster\nimport tempfile\n\ndef generate_map(anomalous_ips, ip_details):\n    \"\"\"\n    Generate an interactive map with markers for anomalous IPs.\n    \n    Args:\n        anomalous_ips (list): List of anomalous IP addresses\n        ip_details (dict): Dictionary with details for each anomalous IP\n        \n    Returns:\n        str: HTML string of the generated map\n    \"\"\"\n    # Create a map centered on the first anomalous IP, or default to world view\n    if anomalous_ips and ip_details:\n        first_ip = anomalous_ips[0]\n        first_details = ip_details[first_ip]\n        center_lat = first_details.get('latitude', 0)\n        center_lon = first_details.get('longitude', 0)\n        \n        # If coordinates are not available, default to world view\n        if center_lat == 0 and center_lon == 0:\n            center_lat = 0\n            center_lon = 0\n            zoom_start = 2\n        else:\n            zoom_start = 4\n    else:\n        # Default to world view if no anomalous IPs found\n        center_lat = 0\n        center_lon = 0\n        zoom_start = 2\n    \n    # Create the map\n    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start, \n                  tiles='OpenStreetMap')\n    \n    # Add title and description\n    title_html = '''\n         <h3 align=\"center\" style=\"font-size:16px\"><b>Anomalous IP Activity Map</b></h3>\n         <p align=\"center\" style=\"font-size:12px\">Red markers indicate potentially malicious activity</p>\n         '''\n    m.get_root().html.add_child(folium.Element(title_html))\n    \n    # Create a marker cluster for better performance with many points\n    marker_cluster = MarkerCluster().add_to(m)\n    \n    # Add markers for each anomalous IP\n    for ip in anomalous_ips:\n        details = ip_details.get(ip, {})\n        lat = details.get('latitude', 0)\n        lon = details.get('longitude', 0)\n        \n        # Skip if no geolocation data\n        if lat == 0 and lon == 0:\n            continue\n        \n        # Create popup content\n        popup_content = f\"\"\"\n        <div style=\"width:250px\">\n            <b>IP Address:</b> {ip}<br>\n            <b>Country:</b> {details.get('country', 'Unknown')}<br>\n            <b>City:</b> {details.get('city', 'Unknown')}<br>\n            <b>Access Count:</b> {details.get('access_count', 0)}<br>\n            <b>Accessed Files:</b><br>\n        \"\"\"\n        \n        # Add list of accessed files\n        accessed_files = details.get('accessed_files', [])\n        if accessed_files:\n            popup_content += \"<ul style='padding-left:20px; margin-top:5px'>\"\n            for file in accessed_files[:10]:  # Limit to 10 files to avoid huge popups\n                popup_content += f\"<li>{file}</li>\"\n            \n            if len(accessed_files) > 10:\n                popup_content += f\"<li>... and {len(accessed_files) - 10} more</li>\"\n            \n            popup_content += \"</ul>\"\n        else:\n            popup_content += \"<p>No file access data available</p>\"\n        \n        popup_content += \"</div>\"\n        \n        # Create and add the marker\n        folium.Marker(\n            location=[lat, lon],\n            popup=folium.Popup(popup_content, max_width=300),\n            tooltip=f\"IP: {ip} - {details.get('country', 'Unknown')}\",\n            icon=folium.Icon(color='red', icon='info-sign')\n        ).add_to(marker_cluster)\n    \n    # Save the map to a temporary HTML file\n    with tempfile.NamedTemporaryFile(suffix='.html', delete=False) as tmp_file:\n        m.save(tmp_file.name)\n        # Read the HTML content\n        with open(tmp_file.name, 'r') as f:\n            html_content = f.read()\n        \n        # Delete the temporary file\n        import os\n        os.unlink(tmp_file.name)\n    \n    return html_content\n","size_bytes":3776}}}